import re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\nimport os\n\ndef parse_ycsb_output(filepath):\n    \"\"\"Parses YCSB output file to extract overall throughput and latency metrics.\"\"\"\n    metrics = {\n        \'Overall\': {\'RunTime(ms)\': None, \'Throughput(ops/sec)\': None},\n        \'UPDATE\': {\'Operations\': None, \'AverageLatency(us)\': None, \'MinLatency(us)\': None, \'MaxLatency(us)\': None,\n                   \'95thPercentileLatency(us)\': None, \'99thPercentileLatency(us)\': None},\n        \'READ\': {\'Operations\': None, \'AverageLatency(us)\': None, \'MinLatency(us)\': None, \'MaxLatency(us)\': None,\n                 \'95thPercentileLatency(us)\': None, \'99thPercentileLatency(us)\': None}\n    }\n    \n    # Simplified parsing focusing on key metrics\n    # This will need to be more robust for actual YCSB output variations\n    try:\n        with open(filepath, \'r\') as f:\n            for line in f:\n                if line.startswith(\'[OVERALL]\, RunTime(ms)\'):\n                    metrics[\'Overall\'][\'RunTime(ms)\'] = float(line.split(\',\')[-1])\n                elif line.startswith(\'[OVERALL]\, Throughput(ops/sec)\'):\n                    metrics[\'Overall\'][\'Throughput(ops/sec)\'] = float(line.split(\',\')[-1])\n                elif line.startswith(\'[UPDATE]\, Operations\'):\n                    metrics[\'UPDATE\'][\'Operations\'] = int(line.split(\',\')[-1])\n                elif line.startswith(\'[UPDATE]\, AverageLatency(us)\'):\n                    metrics[\'UPDATE\'][\'AverageLatency(us)\'] = float(line.split(\',\')[-1])\n                elif line.startswith(\'[READ]\, Operations\'):\n                    metrics[\'READ\'][\'Operations\'] = int(line.split(\',\')[-1])\n                elif line.startswith(\'[READ]\, AverageLatency(us)\'):\n                    metrics[\'READ\'][\'AverageLatency(us)\'] = float(line.split(\',\')[-1])\n    except FileNotFoundError:\n        print(f\"Error: File not found {filepath}\")\n        return None # Or return a default metrics structure\n    except Exception as e:\n        print(f\"Error parsing {filepath}: {e}\")\n        return None\n\n    return metrics\n\ndef generate_plot(all_metrics, output_image_path=\'latency_comparison_plot.png\'):\n    \"\"\"Generates a box and whiskers plot for READ and UPDATE latencies.\"\"\"\n    labels = []\n    read_latencies_avg = []\n    update_latencies_avg = []\n\n    for db_name, metrics_data in all_metrics.items():\n        if metrics_data and metrics_data[\'READ\'][\'AverageLatency(us)\'] is not None and metrics_data[\'UPDATE\'][\'AverageLatency(us)\'] is not None:\n            labels.append(db_name)\n            read_latencies_avg.append(metrics_data[\'READ\'][\'AverageLatency(us)\'])\n            update_latencies_avg.append(metrics_data[\'UPDATE\'][\'AverageLatency(us)\'])\n        else:\n            print(f\"Warning: Missing latency data for {db_name}, skipping in plot.\")\n\n    if not labels: # No data to plot\n        print(\"No data available to generate plot.\")\n        # Create a placeholder image or skip image generation\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, \"No data to plot\", ha=\'center\', va=\'center\')\n        plt.savefig(output_image_path)\n        plt.close()\n        return\n\n    x = np.arange(len(labels))\n    width = 0.35\n\n    fig, ax = plt.subplots(figsize=(12, 8))\n    rects1 = ax.bar(x - width/2, read_latencies_avg, width, label=\'READ Avg Latency (us)\')\n    rects2 = ax.bar(x + width/2, update_latencies_avg, width, label=\'UPDATE Avg Latency (us)\')\n\n    ax.set_ylabel(\'Average Latency (us)\')\n    ax.set_title(\'YCSB Workload A: Average Latency Comparison\')\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels)\n    ax.legend()\n\n    ax.bar_label(rects1, padding=3)\n    ax.bar_label(rects2, padding=3)\n\n    fig.tight_layout()\n    plt.savefig(output_image_path)\n    plt.close()\n    print(f\"Plot saved to {output_image_path}\")\n\ndef generate_markdown_report(all_metrics, plot_image_path=\'latency_comparison_plot.png\', output_md_path=\'benchmark_comparison.md\'):\n    \"\"\"Generates a Markdown report summarizing the benchmark results.\"\"\"\n    with open(output_md_path, \'w\') as md_file:\n        md_file.write(\"# YCSB Benchmark Comparison\\n\\n\")\n        md_file.write(\"This report summarizes the YCSB Workload A performance for different databases.\\n\\n\")\n        \n        md_file.write(\"## Overall Performance Metrics\\n\\n\")\n        md_file.write(\"| Database | Throughput (ops/sec) | RunTime (ms) |\\n\")\n        md_file.write(\"|----------|----------------------|--------------|\\n\")\n        for db_name, metrics_data in all_metrics.items():\n            if metrics_data and metrics_data[\'Overall\'][\'Throughput(ops/sec)\'] is not None:\n                throughput = metrics_data[\'Overall\'][\'Throughput(ops/sec)\']\n                runtime = metrics_data[\'Overall\'][\'RunTime(ms)\']\n                md_file.write(f\"| {db_name} | {throughput:.2f} | {runtime} |\\n\")\n            else:\n                md_file.write(f\"| {db_name} | N/A | N/A |\\n\")\n        md_file.write(\"\\n\")\n\n        md_file.write(\"## Latency Comparison (Average)\\n\\n\")\n        md_file.write(f\"![Latency Comparison Plot]({os.path.basename(plot_image_path)})\\n\\n\")\n        md_file.write(\"The plot above shows average latencies for READ and UPDATE operations. \"\n                        \"For a full box-and-whiskers plot including 25th, 50th (median), and 75th percentiles, \"\n                        \"YCSB needs to be run with `measurementtype=hdrhistogram` or `timeseries` and the output parsed accordingly.\\n\\n\")\n\n        md_file.write(\"### Detailed Average Latencies (us)\\n\\n\")\n        md_file.write(\"| Database | READ Avg Latency (us) | UPDATE Avg Latency (us) |\\n\")\n        md_file.write(\"|----------|-----------------------|-------------------------|\\n\")\n        for db_name, metrics_data in all_metrics.items():\n            if metrics_data and metrics_data[\'READ\'][\'AverageLatency(us)\'] is not None and metrics_data[\'UPDATE\'][\'AverageLatency(us)\'] is not None:\n                read_avg_lat = metrics_data[\'READ\'][\'AverageLatency(us)\']\n                update_avg_lat = metrics_data[\'UPDATE\'][\'AverageLatency(us)\']\n                md_file.write(f\"| {db_name} | {read_avg_lat:.2f} | {update_avg_lat:.2f} |\\n\")\n            else:\n                md_file.write(f\"| {db_name} | N/A | N/A |\\n\")\n        md_file.write(\"\\n\")\n        print(f\"Markdown report saved to {output_md_path}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python generate_report.py <results_file1.txt> [<results_file2.txt> ...] \")\n        print(\"The script expects result file paths as arguments. The base name of the file (e.g., \'cockroachdb\') will be used as the database name.\")\n        sys.exit(1)\n\n    all_database_metrics = {}\n    result_files = sys.argv[1:]\n\n    for f_path in result_files:\n        if not os.path.exists(f_path):\n            print(f\"Error: Result file {f_path} not found. Skipping.\")\n            # Add a placeholder so the report/plot generation doesn't completely fail if one file is missing\n            db_identifier = os.path.basename(f_path).split(\'_\')[0].capitalize()\n            all_database_metrics[db_identifier] = None # Mark as None if file not found\n            continue\n        \n        # Derive database name from filename, e.g., \"cockroachdb_results.txt\" -> \"CockroachDB\"\n        db_name_parts = os.path.basename(f_path).split(\'_\') \n        db_identifier = db_name_parts[0].capitalize()\n        if \"foundationdb\" in f_path.lower(): # Special handling for fdb\n            db_identifier = \"FoundationDB\"\n        elif \"cockroachdb\" in f_path.lower():\n            db_identifier = \"CockroachDB\"\n        elif \"postgresql\" in f_path.lower():\n            db_identifier = \"PostgreSQL\"\n        elif \"sqlite\" in f_path.lower():\n            db_identifier = \"SQLite\"\n\n        metrics = parse_ycsb_output(f_path)\n        all_database_metrics[db_identifier] = metrics\n\n    # Define the order for the report and plot\n    db_order = [\"PostgreSQL\", \"CockroachDB\", \"FoundationDB\", \"SQLite\"]\n    ordered_metrics = {name: all_database_metrics.get(name) for name in db_order if name in all_database_metrics}\n    # Add any other dbs not in the predefined order (in case of new additions)\n    for name, metrics_data in all_database_metrics.items():\n        if name not in ordered_metrics:\n            ordered_metrics[name] = metrics_data\n\n    plot_file = \'latency_comparison_plot.png\'\n    report_file = \'benchmark_comparison.md\'\n\n    generate_plot(ordered_metrics, output_image_path=plot_file)\n    generate_markdown_report(ordered_metrics, plot_image_path=plot_file, output_md_path=report_file)\n\n    print(f\"Generated report: {report_file}\")\n    print(f\"Generated plot: {plot_file}\")\n